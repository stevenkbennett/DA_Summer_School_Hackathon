{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1748e0a",
   "metadata": {},
   "source": [
    "# FoNS Datathon 2021 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353faea0",
   "metadata": {},
   "source": [
    "### Looping through all training data to get the best outcome for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81f0a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17665d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptors = pd.read_csv(\"train_descriptors.csv\")\n",
    "train_mord3d = pd.read_csv(\"train_mord3d.csv\")\n",
    "train_morgan = pd.read_csv(\"train_morgan.csv\")\n",
    "train_rdk = pd.read_csv(\"train_rdk.csv\")\n",
    "\n",
    "train_crystals = pd.read_csv(\"train_crystals.csv\")\n",
    "train_distances = pd.read_csv(\"train_distances.csv\")\n",
    "train_centroid_distances = pd.read_csv(\"train_centroid_distances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20c5cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descriptors = pd.read_csv(\"test_descriptors.csv\")\n",
    "test_mord3d = pd.read_csv(\"test_mord3d.csv\")\n",
    "test_morgan = pd.read_csv(\"test_morgan.csv\")\n",
    "test_rdk = pd.read_csv(\"test_rdk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db434242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rdk(train_data, test_data, target):\n",
    "    \n",
    "    # Drop first column - no columns should have NA values\n",
    "    train_data_full = train_data.iloc[:, 1:]\n",
    "    \n",
    "    # Make test set have same columns as training set\n",
    "    test_data_full = test_data[train_data_full.columns]\n",
    "    \n",
    "    # Create training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_data_full,\n",
    "                                                                              target,\n",
    "                                                                              random_state = 0)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid, train_data_full, test_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "324d8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_morgan(train_data, test_data, target): \n",
    "    \n",
    "    # Drop first column - no columns should have NA values\n",
    "    train_data_full = train_data.iloc[:, 1:]\n",
    "    \n",
    "    # Make test set have same columns as training set\n",
    "    test_data_full = test_data[train_data_full.columns]\n",
    "    \n",
    "    # Create training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_data_full,\n",
    "                                                                              target,\n",
    "                                                                              random_state = 0)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid, train_data_full, test_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1707c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mord3d(train_data, test_data, target, pca=False):\n",
    "    \n",
    "    \n",
    "    # Are there any redundant columns (all values the same)? These wouldn't contribute to the model at all\n",
    "    for col in train_data.columns:\n",
    "        if len(train_data[col].unique()) == 1:\n",
    "            # If so, drop redundant columns:\n",
    "            train_data.drop(col,inplace=True,axis=1)\n",
    "                               \n",
    "    # Drop all columns with NA - and any non-numerical data\n",
    "    train_data_full = train_data.iloc[:, 2:-4].dropna(axis= 1, how=\"any\")\n",
    "    \n",
    "    # Make test set have same columns as training set\n",
    "    test_data_full = test_data[train_data_full.columns]\n",
    "    \n",
    "     # Perform PCA\n",
    "    if pca == True:\n",
    "        train_PCA = decomposition.PCA(n_components=.95)\n",
    "        scaler_for_PCA = preprocessing.StandardScaler()\n",
    "        train_data_full = train_PCA.fit_transform(scaler_for_PCA.fit_transform(train_data_full))\n",
    "        test_data_full = train_PCA.transform(scaler_for_PCA.transform(test_data_full))\n",
    "        \n",
    "        # Create training and validation sets\n",
    "        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_data_full,\n",
    "                                                                              target,\n",
    "                                                                              random_state = 0)\n",
    "    \n",
    "    else:\n",
    "        # Create training and validation sets\n",
    "        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_data_full,\n",
    "                                                                              target,\n",
    "                                                                              random_state = 0)\n",
    "\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid, train_data_full, test_data_full\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f26af433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_descriptors(train_data, test_data, target, pca=False):\n",
    "    \n",
    "    # Are there any redundant columns (all values the same)? These wouldn't contribute to the model at all\n",
    "    for col in train_data.columns:\n",
    "        if len(train_data[col].unique()) == 1:\n",
    "            # If so, drop redundant columns:\n",
    "            train_data.drop(col,inplace=True,axis=1)\n",
    "                               \n",
    "    # Drop all columns with NA - Last 2 columns are InchiKey and SMILES\n",
    "    train_data_full = train_data.iloc[:, 3:-2].dropna(axis= 1, how=\"any\")\n",
    "    \n",
    "    # Make test set have same columns as training set\n",
    "    test_data_full = test_data[train_data_full.columns]\n",
    "    \n",
    "    # Perform PCA\n",
    "    if pca == True:\n",
    "        train_PCA = decomposition.PCA(n_components=.95)\n",
    "        scaler_for_PCA = preprocessing.StandardScaler()\n",
    "        train_data_full = train_PCA.fit_transform(scaler_for_PCA.fit_transform(train_data_full))\n",
    "        test_data_full = train_PCA.transform(scaler_for_PCA.transform(test_data_full))\n",
    "        \n",
    "        # Create training and validation sets\n",
    "        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_data_full,\n",
    "                                                                              target,\n",
    "                                                                              random_state = 0)\n",
    "    \n",
    "    else:\n",
    "        # Create training and validation sets\n",
    "        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(train_data_full,\n",
    "                                                                              target,\n",
    "                                                                              random_state = 0)\n",
    "\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid, train_data_full, test_data_full\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6cc8c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction target\n",
    "prediction_target = \"calculated_density\"\n",
    "target= train_crystals[prediction_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f75b6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all datasets\n",
    "train_descriptors_list = preprocess_descriptors(train_descriptors,\n",
    "                                                test_descriptors,\n",
    "                                                target,\n",
    "                                                pca=True)\n",
    "\n",
    "train_mord3d_list = preprocess_mord3d(train_mord3d,\n",
    "                                     test_mord3d,\n",
    "                                     target, \n",
    "                                     pca=True)\n",
    "\n",
    "train_morgan_list = preprocess_morgan(train_morgan,\n",
    "                                     test_morgan,\n",
    "                                     target)\n",
    "\n",
    "train_rdk_list = preprocess_rdk(train_rdk,\n",
    "                               test_rdk,\n",
    "                               target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "70c27879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all the processed datasets\n",
    "to_loop = [train_descriptors_list, train_mord3d_list, train_morgan_list,  train_rdk_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d2cecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_descriptors(list_of_descriptor_sets, cv_folds):\n",
    "    \n",
    "    '''This function loops through the list of processed datasets and outputs\n",
    "    the CV score for each one in a list.'''\n",
    "    \n",
    "    # Set up an algorithm \n",
    "    clf = ensemble.RandomForestRegressor(n_estimators=190, max_depth=27.3442, n_jobs=-1)\n",
    "    \n",
    "    # Loop through the datasets and calculate CV score\n",
    "    cv_scores = []\n",
    "    for dataset in list_of_descriptor_sets:\n",
    "            \n",
    "            clf.fit(dataset[0], dataset[2])\n",
    "            score = model_selection.cross_val_score(clf, dataset[0],\n",
    "                                            dataset[2], \n",
    "                                            n_jobs=-1,\n",
    "                                            scoring=\"neg_mean_absolute_error\",\n",
    "                                            cv=cv_folds).mean()\n",
    "            cv_scores.append(score)\n",
    "            \n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf9fb699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.06129466782960302,\n",
       " -0.08131751394105391,\n",
       " -0.08473092666708561,\n",
       " -0.1278201188152448]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_descriptors(to_loop, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
